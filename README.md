<div align="center" dir="auto">

![Research Logo](https://github.com/shonshchori/The-Sound-of-Emotions/assets/83012453/13743b58-3d9e-48ca-8899-570792a40db5)

</div>

<br>

Music in the current digital era is consumed primarily on-demand, for example by streaming from Spotify or YouTube, rather than by broadcasting, for example by listening to a radio station. A strong correlation has been found between music and emotions. Hence, with our ability to select our own music and the availability of artificial intelligence (AI) tools, the importance of music emotion recognition (MER) is significant. This research proposes a novel methodology for predicting emotions from a musical piece based on a two-layer AI. The dataset and the process are exceptional in two ways: first, by their qualitative character, which minimized the inherent subjectivity of self-reported emotions, and also the high reliability of the collected data in comparison to other methods such as a questionnaire; and second by their quantitative character, given n=9,090 songs from about 4,447 people. The model succeeded in predicting the primary emotion with accuracy=57%  and one of the two leading emotions (the primary and the secondary) with accuracy=75.1%, which is significantly higher than a random guess, and without accommodating demographic factors in the model. We believe this knowledge may be a useful tool for therapists, coaches, teachers, etc. and may also raise public awareness of a potential privacy violation. 

